{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"left\">**Hypothesis Testing** </h1>\n",
    "<br>\n",
    "<img src=\"../images/hypothesis.jpg\" alt=\"Python\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John's Curiosity of OldTown \n",
    "***\n",
    " - Now, John is interested in dissecting the whole Real Estate Scene in OldTown\n",
    " \n",
    " - He's interested in seeing whether the prices of houses are different (on an average) when compared to the rest of Brooklyn \n",
    " \n",
    " - John has just conjured up what is famously known as a \"Hypothesis\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "## Hypothesis \n",
    "***\n",
    "A statement that might be true, which can then be tested.\n",
    "\n",
    "Example: Sam has a hypothesis that \"large dogs are better at catching tennis balls than small dogs\". We can test that hypothesis by having hundreds of different sized dogs try to catch tennis balls.\n",
    "\n",
    "- The beauty of these Hypotheses are that they can be TESTED! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing \n",
    "***\n",
    "- Statistical hypothesis tests are based a statement called the null hypothesis that assumes nothing interesting is going on between whatever variables you are testing. \n",
    " \n",
    " - Therefore, in John's case the Null Hypothesis is that:\n",
    "     - \"The Mean of House Prices in OldTown is not different from the Houses all over Brooklyn\n",
    "     \n",
    " ### Why Null Hypothesis? \n",
    " - The purpose of a hypothesis test is to determine whether the null hypothesis is likely to be true given sample data.\n",
    " - If there is little evidence against the null hypothesis given the data, you accept the null hypothesis.\n",
    " - If the null hypothesis is unlikely given the data, you might reject the null in favor of the alternative hypothesis: that something interesting is going on.\n",
    " \n",
    " ### Alternative Hypothesis\n",
    " - This is nothing but the question you ask which kind of \"opposes\" the Null Hypothesis\n",
    " \n",
    " - Therefore, in John's case the Alternative Hypothesis is that:\n",
    "     - \"The Mean of House Prices in OldTown **IS** different from the Houses all over Brooklyn\n",
    "     \n",
    " - Only 1 Hypothesis can be right\n",
    " \n",
    " - In hypothesis testing we test a sample, with the goal of accepting or rejecting a null hypothesis which is our assumption or the default position. The test tells us whether or not our primary hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "***\n",
    "\n",
    "### The null hypothesis is assumed true and statistical evidence is required to reject it in favor of a research or alternative hypothesis\n",
    "\n",
    " - We require a standard on the available evidence to reject the null hypothesis (convict)\n",
    "\n",
    "\n",
    "If we set a low standard\n",
    ", then we would increase the percentage of innocent people convicted\n",
    "; however we would also increase the percentage of guilty people convicted\n",
    "(correctly rejecting the null)\n",
    "\n",
    "\n",
    "If we set a high standard, then we increase the the percentage of innocent people let free\n",
    " while we would also increase the percentage of guilty people let free\n",
    "(type II errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/Maths-Insight.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "## Math behind Hypothesis Testing\n",
    "***\n",
    "Once you have the null and alternative hypothesis in hand, you choose a significance level (often denoted by the Greek letter α). \n",
    "\n",
    " - The significance level is a probability threshold that determines when you reject the null hypothesis.\n",
    " <img src=\"../images/sig.jpg\" alt=\"Significance Level\" style=\"width: 600px;float:left; margin-right:15px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- So we use this to calculate a \"Test Statistic\" that would further help us do further calculations\n",
    " ***\n",
    " <center><img src=\"../images/hyp1.png\" alt=\"Drawing\" style=\"width: 350px;\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - After carrying out a test, if the probability of getting a result as extreme as the one you observe due to chance is lower than the significance level, you reject the null hypothesis in favor of the alternative. \n",
    " \n",
    " - This probability of seeing a result as extreme or more extreme than the one observed is known as the *p-value*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of p-value\n",
    "***\n",
    "- The p-value is really not as complicated as people make it sound\n",
    "\n",
    "- So now say that we have put a significance (alpha) = 0.05\n",
    "    - This means that if we see a p-value of lesser than 0.05, we reject our Null and accept the Alternative to be true \n",
    "    \n",
    "    - What you have to understand is the data from your Null hypothesis follows a distribution (Normally distributed) \n",
    "     - Just imagine 1 bell curve of the data from the Null Hypothesis\n",
    "     - Now imagine another bell curve which hypothetically defines your Alternative Hypothesis\n",
    "     \n",
    "     See below: \n",
    "        \n",
    " ***\n",
    " <center><img src=\"../images/pval1.png\" alt=\"Drawing\" style=\"width: 350px;\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    " - So what the p-value says is that it is the Probability of finding the Alternative Hypothesis data in the Null Hypothesis data (bell curve 1!!) \n",
    " \n",
    " - If it is lesser than 0.05(our threshold) then we reject it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Why reject it though? \n",
    "\n",
    " - BECAUSE OUR ALTERNATIVE HYPOTHESIS DATA IS REAL! NO ONE MADE IT UP! IT IS LEGIT DATA THAT IS OBSERVED AND NOT JUST FAKE! \n",
    " - So if it is real, we can say that such data isn't really described by the Null Hypothesis (Bell Curve 1) therefore the Null must be rejected as being TRUE! \n",
    " \n",
    " - It now makes sense! P-values are cool again "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well....\n",
    "\n",
    " ***\n",
    " <center><img src=\"../images/pval_meme.png\" alt=\"Drawing\" style=\"width: 350px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming back to John \n",
    "***\n",
    " - Let's see how John did now\n",
    " - Are house prices in OldTown really different from the House Prices of Brooklyn? \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_csv(\"../train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-statistic is :-10.639294263334575\n",
      "P-value is :1.9560526026260018e-26\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.weightstats import ztest\n",
    "z_statistic, p_value = ztest(x1=data[data['Neighborhood'] == 'OldTown']['SalePrice'], value=data['SalePrice'].mean())\n",
    "print('Z-statistic is :{}'.format(z_statistic))\n",
    "print('P-value is :{}'.format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the p-value\n",
    "***\n",
    "* When performing a hypothesis test, the p-value is the probability of given or more extreme outcome given the null-hypothesis is true.\n",
    "\n",
    "* We see that the p-value is close to zero i.e., the probability of getting the given distribution of houseprices in OldTown under the assumption that it its mean is the same as the mean of all house prices.\n",
    "* So what can we infer from the p-value of our test? What should be the p-value beyond which we reject the null hypothesis.\n",
    "* The p-value below which we reject our hypothesis depends on our **significance level** $\\alpha$\n",
    "* For a 95% signifigance level we reject our null hypothesis if p-value is below 0.05\n",
    "* In this case we can reject the null hypothesis at 95% significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to test: Student's t-test\n",
    "***\n",
    "* The T-test is a statistical test used to determine whether a numeric data sample of differs significantly from the population or whether two samples differ from one another.\n",
    "* A z-test assumes a sample size >30 to work, but what if our sample is less than 30?\n",
    "* A t-test solves this problem and gives us a way to do a hypothesis test on a smaller sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oh John\n",
    "***\n",
    "- Now, John also wants to see if house prices in `Stone Brook` neighborhood are different from the rest of the Houses in Brooklyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of houses in Stone Brook: 25\n"
     ]
    }
   ],
   "source": [
    "print('No of houses in Stone Brook: {}'\\\n",
    "      .format(data['Neighborhood'].value_counts()['StoneBr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets do a t-test to test our hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=5.735070151700397, pvalue=6.558704101036394e-06)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_1samp(a= data[data['Neighborhood'] == 'StoneBr']['SalePrice'],               # Sample data\n",
    "                 popmean= data['SalePrice'].mean())  # Pop mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The p-value in this case again is low and we can reject our null hypothesis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "### Type I and Type II Error\n",
    "***\n",
    "* If we again think of hypothesis test as a criminal trial then it makes sense to frame the verdict in terms of null and alternate hypothesis:\n",
    "\n",
    "[Trial](images/jury.png)\n",
    "    * Null Hypothesis: Defendant is innocent\n",
    "    * Alternate Hypothesis: Defendant is guilty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "* What type of error is being committed in the following circumstances?\n",
    "    * Declaring the defendant guilty when they are actually innocent?\n",
    "    * Declaring the defendant innocent when they are actually guilty?\n",
    "    \n",
    "* The first one is a type I error also known as a \"false positive\" or \"false hit\".\n",
    "* The type 1 error rate is equal to the significance level α, so setting a higher confidence level (and therefore lower alpha) reduces the chances of getting a false positive.\n",
    "\n",
    "* The second one is a type I error also known as a \"false negative\" or \"miss\". The higher your confidence level, the more likely you are to make a type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><img src=\"../images/hyp2.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***\n",
    " <center><img src=\"../images/hyp3.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 1 Error \n",
    "***\n",
    "Type I error describes a situation where you reject the null hypothesis when it is actually true. \n",
    "\n",
    "This type of error is also known as a false positive or false hit.\n",
    "\n",
    "The type 1 error rate is equal to the significance level α, so setting a higher confidence level (and therefore lower alpha) reduces the chances of getting a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 2 error\n",
    "***\n",
    "Type II error describes a situation where you fail to reject the null hypothesis when it is actually false. \n",
    "\n",
    "Type II error is also known as a false negative or miss. The higher your confidence level, the more likely you are to make a type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Squared Goodness-Of-Fit Test\n",
    "***\n",
    "A chi-squared goodness of fit tests whether the distribution of sample categorical data matches an expected distribution. \n",
    "\n",
    "* For example, you could use a chi-squared goodness-of-fit test to check whether the race demographics of members at your church or school match that of the entire U.S. population or whether the computer browser preferences of your friends match those of Internet uses as a whole.\n",
    "\n",
    "When working with categorical data the values the observations themselves aren't of much use for statistical testing because categories like \"male\", \"female,\" and \"other\" have no mathematical meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Tests dealing with categorical variables are based on variable counts instead of the actual value of the variables themselves.\n",
    "\n",
    "Let's generate some fake demographic data for U.S. and Minnesota and walk through the chi-square goodness of fit test to check whether they are different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "National\n",
      "col_0      count\n",
      "0               \n",
      "asian      15000\n",
      "black      50000\n",
      "hispanic   60000\n",
      "other      35000\n",
      "white     100000\n",
      " \n",
      "Minnesota\n",
      "col_0     count\n",
      "0              \n",
      "asian        75\n",
      "black       250\n",
      "hispanic    300\n",
      "other       150\n",
      "white       600\n"
     ]
    }
   ],
   "source": [
    "national = pd.DataFrame([\"white\"]*100000 + [\"hispanic\"]*60000 +\\\n",
    "                        [\"black\"]*50000 + [\"asian\"]*15000 + [\"other\"]*35000)          \n",
    "\n",
    "minnesota = pd.DataFrame([\"white\"]*600 + [\"hispanic\"]*300 + \\\n",
    "                         [\"black\"]*250 +[\"asian\"]*75 + [\"other\"]*150)\n",
    "\n",
    "national_table = pd.crosstab(index=national[0], columns=\"count\")\n",
    "minnesota_table = pd.crosstab(index=minnesota[0], columns=\"count\")\n",
    "\n",
    "print( \"National\")\n",
    "print(national_table)\n",
    "print(\" \")\n",
    "print( \"Minnesota\")\n",
    "print(minnesota_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-squared tests are based on the so-called chi-squared statistic. You calculate the chi-squared statistic with the following formula:\n",
    "\n",
    ">$sum((observed−expected)^2/expected)$\n",
    "\n",
    "\n",
    "In the formula, observed is the actual observed count for each category and expected is the expected count based on the distribution of the population for the corresponding category. \n",
    "\n",
    "Let's calculate the chi-squared statistic for our data to illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0\n",
      "count    18.194805\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "observed = minnesota_table\n",
    "\n",
    "national_ratios = national_table/len(national)  # Get population ratios\n",
    "\n",
    "expected = national_ratios * len(minnesota)   # Get expected counts\n",
    "\n",
    "chi_squared_stat = (((observed-expected)**2)/expected).sum()\n",
    "\n",
    "print(chi_squared_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The chi-squared test assumes none of the expected counts are less than 5.\n",
    "\n",
    "Similar to the t-test where we compared the t-test statistic to a critical value based on the t-distribution to determine whether the result is significant, in the chi-square test we compare the chi-square test statistic to a critical value based on the chi-square distribution. \n",
    "\n",
    "The scipy library shorthand for the chi-square distribution is chi2. \n",
    "\n",
    "Let's use this knowledge to find the critical value for 95% confidence level and check the p-value of our result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical value\n",
      "9.487729036781154\n",
      "P value\n",
      "[0.00113047]\n"
     ]
    }
   ],
   "source": [
    "crit = stats.chi2.ppf(q = 0.95, # Find the critical value for 95% confidence*\n",
    "                      df = 4)   # Df = number of variable categories - 1\n",
    "\n",
    "print(\"Critical value\")\n",
    "print(crit)\n",
    "\n",
    "p_value = 1 - stats.chi2.cdf(x=chi_squared_stat,  # Find the p-value\n",
    "                             df=4)\n",
    "print(\"P value\")\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** we are only interested in the right tail of the chi-square distribution. Read more on this [here](https://en.wikipedia.org/wiki/Chi-squared_distribution).\n",
    "\n",
    "Since our chi-squared statistic exceeds the critical value, we'd reject the null hypothesis that the two distributions are the same.\n",
    "\n",
    "You can carry out a chi-squared goodness-of-fit test automatically using the scipy function scipy.stats.chisquare():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=array([18.19480519]), pvalue=array([0.00113047]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.chisquare(f_obs= observed,   # Array of observed counts\n",
    "                f_exp= expected)   # Array of expected counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results agree with the values we calculated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "### Chi-Squared Test of Independence\n",
    "***\n",
    "Independence is a key concept in probability that describes a situation where knowing the value of one variable tells you nothing about the value of another. \n",
    "\n",
    "For instance, the month you were born probably doesn't tell you anything which web browser you use, so we'd expect birth month and browser preference to be independent. \n",
    "\n",
    "On the other hand, your month of birth might be related to whether you excelled at sports in school, so month of birth and sports performance might not be independent.\n",
    "\n",
    "The chi-squared test of independence tests whether two categorical variables are independent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John's final experiments\n",
    "***\n",
    "John wants to test if knowing `LandContour` which is the overall flatness of the property tells him anything about the price\n",
    "\n",
    " -  He has divided the `SalePrice` in three buckets - High, medium, low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sp\n",
    "def compute_freq_chi2(x,y):\n",
    "    \"\"\"This function will compute frequency table of x an y\n",
    "    Pandas Series, and use the table to feed for the contigency table\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    x,y : Pandas Series, must be same shape for frequency table\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    None. But prints out frequency table, chi2 test statistic, and \n",
    "    p-value\n",
    "    \"\"\"\n",
    "    freqtab = pd.crosstab(x,y)\n",
    "    print(\"Frequency table\")\n",
    "    print(\"============================\")\n",
    "    print(freqtab)\n",
    "    print(\"============================\")\n",
    "    chi2,pval,dof,expected = sp.chi2_contingency(freqtab)\n",
    "    print(\"ChiSquare test statistic: \",chi2)\n",
    "    print(\"p-value: \",pval)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency table\n",
      "============================\n",
      "SalePrice    High  Medium  Low\n",
      "LandContour                   \n",
      "Bnk            32      20   11\n",
      "HLS            10      12   28\n",
      "Low             8      11   17\n",
      "Lvl           437     447  427\n",
      "============================\n",
      "ChiSquare test statistic:  26.252544346201447\n",
      "p-value:  0.00019976918050008285\n"
     ]
    }
   ],
   "source": [
    "price = pd.qcut(data['SalePrice'], 3, labels = ['High', 'Medium', 'Low'])\n",
    "compute_freq_chi2(data.LandContour, price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The low p-value tells us that the two variables aren't independent and knowing the `LandContour` of a house does tells us something about its `SalePrice`.\n",
    "* The frequency distribution reflects this.\n",
    "* Houses that are Near Flat/Level(Lvl) have an equal distribution of `SalePrice`.\n",
    "* On the other hand houses that are at a Hillside i.e., Significant slope from side to side (HLS) have almost thrice as much houses with low price than high prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/Recap.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "# Recap Time\n",
    "***\n",
    "* Hyothesis Testing\n",
    "* Type 1 & Type 2 error\n",
    "* Chi-Squared Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You\n",
    "***\n",
    "For more queries - Reach out to vikash.sharma@msci.com "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
